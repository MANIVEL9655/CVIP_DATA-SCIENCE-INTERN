import os
import pickle
import numpy as np
from tqdm.notebook import tqdm
import tensorflow as tf
from tensorflow.keras.applications.vgg16 import VGG16 , preprocess_input
from tensorflow.keras.preprocessing.image import load_img , img_to_array
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical , plot_model
from nltk.translate.bleu_score import corpus_bleu
base_dir = '/kaggle/input/flickr8k'
working_dir = '/kaggle/working'
pre_trained_model = VGG16()
feature_extractor = Model(inputs = pre_trained_model.inputs , 
outputs = pre_trained_model.layers[-2].output)
feature_extractor.summary()
features = {}
directory = os.path.join(base_dir , 'Images')
for img_name in tqdm(os.listdir(directory)):
    img_path = os.path.join(directory , img_name)
    image = load_img(img_path , target_size=(224,224))
    image = img_to_array(image)
    image = image.reshape(1 , image.shape[0] , image.shape[1] , image.shape[2])
    image = preprocess_input(image)
    feature = feature_extractor.predict(image , verbose=0)
    image_id = img_name.split('.')[0]
    features[image_id] = feature
pickle.dump(features , open(os.path.join(working_dir ,'img_features.pkl') , 'wb'))
with open(os.path.join(working_dir , 'img_features.pkl') , 'rb') as f:
    features = pickle.load(f)
with open(os.path.join(base_dir , 'captions.txt') , 'r') as f:
    next(f)
    captions_doc = f.read()
mapping = {}
for line in tqdm(captions_doc.split("\n"))
    tokens = line.split(',')
    if len(line)<2:
        continue   
    img_id , caption = tokens[0] , tokens[1:] 
    img_id = img_id.split('.')[0]
    caption = " ".join(caption) 
    if img_id not in mapping:
        mapping[img_id] = [] 
    mapping[img_id].append(caption)
len(mapping)
def clean(mapping):
    for key , captions in mapping.items():
        for i in range(len(captions)):
            caption = captions[i]
            caption = caption.lower()
            caption = caption.replace('[^A-Za-z]' , '')
            caption = caption.replace('\s+' ,' ')
            caption = 'startseq ' + " ".join([word for word in caption.split() if len(word)>1]) + ' endseq'
            captions[i] = caption
mapping['1000268201_693b08cb0e']
clean(mapping)
mapping['1002674143_1b742ab4b8']
all_captions = []
for key in mapping:
    for captions in mapping[key]:
        all_captions.append(captions)
len(all_captions)
print(all_captions[:10])
tokenizer = Tokenizer()
tokenizer.fit_on_texts(all_captions)
vocab_size = len(tokenizer.word_index) + 1
print(vocab_size)
with open(os.path.join(working_dir,'tokenizer.pkl'), 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
with open(os.path.join(working_dir,'tokenizer.pkl'), 'rb') as handle:
    tokenizer = pickle.load(handle)
max_length = max(len(captions.split()) for captions in all_captions)
print(max_length)
image_ids = list(mapping.keys())
split = int(len(image_ids) * 0.9)
train = image_ids[:split]
test = image_ids[split:]
print(split)
def data_generator(data_keys , mapping , features , tokenizer , max_length , vocab_size , batch_size):
    x1 , x2 , y = list() , list() , list()
    n =0
    while 1:
        for key in data_keys:
            n+=1
            captions = mapping[key]
            for caption in captions:
                seq = tokenizer.texts_to_sequences([caption])[0]
                for i in range(1,len(seq)):
                    in_seq , out_seq = seq[:i] , seq[i]
                    in_seq = pad_sequences([in_seq] , maxlen=max_length)[0]
                    out_seq = to_categorical([out_seq] , num_classes=vocab_size)[0]
                
                    x1.append(features[key][0]) 
                    x2.append(in_seq) 
                    y.append(out_seq) 
                
            if n == batch_size:
                x1 , x2 , y = np.array(x1) , np.array(x2) , np.array(y)
                yield [x1,x2] , y
                x1 , x2 , y = list() , list() , list()
                n = 0
inputs1 = tf.keras.layers.Input(shape = (4096 ,))
el1 = tf.keras.layers.Dropout(0.4)(inputs1)
el2 = tf.keras.layers.Dense(256 , activation='relu')(el1)
inputs2 = tf.keras.layers.Input(shape=(max_length ,))
dl1 = tf.keras.layers.Embedding(vocab_size , 256 , mask_zero=True)(inputs2)
dl2 = tf.keras.layers.Dropout(0.4)(dl1)
dl3 = tf.keras.layers.LSTM(256)(dl2)
dl4 = tf.keras.layers.add([el2 , dl3])
dl5 = tf.keras.layers.Dense(256 , activation='relu')(dl4)
outputs = tf.keras.layers.Dense(vocab_size , activation='softmax')(dl5)
model = Model(inputs = [inputs1 , inputs2]  , outputs = outputs)
inputs1 = tf.keras.layers.Input(shape = (4096 ,))
el1 = tf.keras.layers.Dropout(0.4)(inputs1)
el2 = tf.keras.layers.Dense(256 , activation='relu')(el1)
inputs2 = tf.keras.layers.Input(shape=(max_length ,))
dl1 = tf.keras.layers.Embedding(vocab_size , 256 , mask_zero=True)(inputs2)
dl2 = tf.keras.layers.Dropout(0.4)(dl1)
dl3 = tf.keras.layers.LSTM(256)(dl2)
dl4 = tf.keras.layers.add([el2 , dl3])
dl5 = tf.keras.layers.Dense(256 , activation='relu')(dl4)
outputs = tf.keras.layers.Dense(vocab_size , activation='softmax')(dl5)
model = Model(inputs = [inputs1 , inputs2]  , outputs = outputs)
epochs = 20
batch_size = 64
steps = len(train) // batch_size
for i in range(epochs):
    generator = data_generator(train , mapping , features , tokenizer , max_length , vocab_size , batch_size)
    model.fit(generator , epochs=1 , steps_per_epoch=steps ,verbose=1)
model.save(working_dir + "/caption_gen.h5")
def idx_to_word(integer , tokenizer):
    for word , index in tokenizer.word_index.items():
        if index == integer:
            return word
    return None
def predict_caption(model , image , tokenizer , max_length):
    in_text = 'startseq'
    for i in range(max_length):
        sequence = tokenizer.texts_to_sequences([in_text])[0]
        sequence = pad_sequences([sequence] , max_length)
        y = model.predict([image , sequence] , verbose=0)
        y = np.argmax(y)
        word = idx_to_word(y , tokenizer)
        if word is None:
            break
        in_text = in_text + " " + word
        if word == "endseq":
            break
    
    return in_text
actual , predict = list() , list()
key = '1019077836_6fc9b15408'
captions = mapping[key]
y_pred = predict_caption(model , features[key] , tokenizer , max_length)
y_real = [caption.split() for caption in captions]
y_pred = y_pred.split()  
actual.append(y_real)
predict.append(y_pred)
print('bleu 1 : %f' % corpus_bleu(actual , predict , weights=(1.0 , 0 , 0 , 0)))
print('bleu 2 : %f' % corpus_bleu(actual , predict , weights=(0.5, 0.5 , 0 , 0)))